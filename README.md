# Model Refinement
## A New Approach to Knowledge Distillation
I explore a new pipeline for knowledge distillation that builds off of the HuggingFace approach used to build a lighter version of the larger BERT model [1]. 
## Knowledge Distillation in the Computer Vision Domain
I first explore the transferability of the approach to the domain of computer vision using the landmark ResNet architecture [2].

# References
[1] Sanh, Victor, et al. “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.” ArXiv Preprint ArXiv:1910.01108, 2019.\
[2] He, Kaiming, et al. “Deep Residual Learning for Image Recognition.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.
